---
title: "FastAPI Ã— BigQuery CRUDå®Ÿè£…ã‚¬ã‚¤ãƒ‰ â”€ BigQueryã‚’APIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ä½¿ã†å®Ÿè·µãƒ‘ã‚¿ãƒ¼ãƒ³"
emoji: "ğŸ—„ï¸"
type: "tech"
topics: ["fastapi", "bigquery", "gcp", "python", "api"]
published: false
publication_name: "correlate_dev"
---

## ã¯ã˜ã‚ã«

FastAPI ã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰APIã‚’æ§‹ç¯‰ã™ã‚‹éš›ã€å¤šãã®è¨˜äº‹ã§ã¯ SQLAlchemy + PostgreSQL ã®çµ„ã¿åˆã‚ã›ãŒç´¹ä»‹ã•ã‚Œã¾ã™ã€‚ã—ã‹ã—ã€Google Cloud Platform ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹å ´åˆã€BigQuery ã‚’APIã®ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ã†ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒéå¸¸ã«æœ‰åŠ¹ã§ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€ **FastAPI Ã— google-cloud-bigquery ã‚’ä½¿ã£ã¦CRUD APIã‚’å®Ÿè£…ã™ã‚‹å®Ÿè·µçš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³** ã‚’è§£èª¬ã—ã¾ã™ã€‚

### BigQueryã‚’APIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ä½¿ã†ãƒ¡ãƒªãƒƒãƒˆãƒ»ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ

BigQueryã¯ã‚‚ã¨ã‚‚ã¨åˆ†æç”¨é€”ã®ãƒ‡ãƒ¼ã‚¿ã‚¦ã‚§ã‚¢ãƒã‚¦ã‚¹ã§ã™ãŒã€APIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ã†æ§‹æˆã«ã‚‚é©ã—ãŸã‚·ãƒ¼ãƒ³ãŒã‚ã‚Šã¾ã™ã€‚

**ãƒ¡ãƒªãƒƒãƒˆ**

- **ã‚¤ãƒ³ãƒ•ãƒ©ç®¡ç†ä¸è¦**: DBã‚µãƒ¼ãƒãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãƒ»ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ»ãƒ‘ãƒƒãƒé©ç”¨ãŒä¸è¦ã§ã™
- **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: æ•°TBã®ãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŒã˜APIã‚³ãƒ¼ãƒ‰ã§ã‚¯ã‚¨ãƒªã§ãã¾ã™
- **åˆ†æã¨ã®çµ±åˆ**: BI ãƒ„ãƒ¼ãƒ«ã‚„åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ã®å ´æ‰€ã§ç®¡ç†ã§ãã¾ã™
- **GCPã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¨ã®è¦ªå’Œæ€§**: IAMã€Workload Identityã€Cloud Logging ã¨ã®çµ±åˆãŒå®¹æ˜“ã§ã™

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãƒ»åˆ¶ç´„**

- **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: ã‚¯ã‚¨ãƒªå®Ÿè¡Œã«æ•°ç™¾msã€œæ•°ç§’ã‹ã‹ã‚Šã¾ã™ã€‚ãƒŸãƒªç§’å˜ä½ã®å¿œç­”ãŒå¿…è¦ãªç”¨é€”ã«ã¯ä¸å‘ãã§ã™
- **DMLã‚¯ã‚©ãƒ¼ã‚¿**: 1ãƒ†ãƒ¼ãƒ–ãƒ«ã‚ãŸã‚Š1æ—¥1,500å›ã®DMLï¼ˆINSERT/UPDATE/DELETEï¼‰ä¸Šé™ãŒã‚ã‚Šã¾ã™
- **ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³éå¯¾å¿œ**: è¤‡æ•°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã¾ãŸãã‚¢ãƒˆãƒŸãƒƒã‚¯ãªæ“ä½œã¯é›£ã—ã„ã§ã™
- **ã‚³ã‚¹ãƒˆ**: å¤§é‡ã®å°è¦æ¨¡ã‚¯ã‚¨ãƒªã¯ã‚¹ã‚­ãƒ£ãƒ³é‡ãŒç©ã¿ä¸ŠãŒã‚Šå‰²é«˜ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™

### é©ã—ãŸãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

BigQueryã‚’APIãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ä½¿ã†ã®ãŒé©ã—ã¦ã„ã‚‹ã®ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã§ã™ã€‚

- åˆ†æé›†è¨ˆçµæœã®é–²è¦§APIï¼ˆèª­ã¿å–ã‚Šå¤šã‚ã€æ›¸ãè¾¼ã¿å°‘ãªã‚ï¼‰
- BigQueryã‚’Single Source of Truthã¨ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 
- ãƒ­ã‚°ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®å‚ç…§API
- ãƒãƒƒãƒé›†è¨ˆçµæœã®é…ä¿¡

é€†ã«ã€æ¯ç§’æ•°åƒä»¶ã®æ›¸ãè¾¼ã¿ãŒç™ºç”Ÿã™ã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ APIã«ã¯å‘ãã¾ã›ã‚“ã€‚

### ã“ã®è¨˜äº‹ã§ä½¿ã†æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

| è¦ç´  | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ |
|------|----------|
| Python | 3.12 |
| FastAPI | 0.115.x |
| google-cloud-bigquery | 3.x |
| Pydantic | v2 (2.x) |
| uvicorn | 0.32.x |

---

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

```mermaid
graph LR
    Client["ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ\n(Browser / App)"]
    FastAPI["FastAPI\n(Cloud Run)"]
    BQ["BigQuery\n(GCP)"]

    Client -- "HTTP Request" --> FastAPI
    FastAPI -- "google-cloud-bigquery" --> BQ
    BQ -- "QueryJob Result" --> FastAPI
    FastAPI -- "JSON Response" --> Client

    style FastAPI fill:#4285f4,color:#fff
    style BQ fill:#34a853,color:#fff
```

ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’è©³ã—ãè¦‹ã‚‹ã¨æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```mermaid
sequenceDiagram
    participant C as Client
    participant FA as FastAPI
    participant D as Depends()
    participant BQ as BigQuery

    C->>FA: GET /items?limit=10&offset=0
    FA->>D: get_bq_client()
    D-->>FA: bigquery.Client
    FA->>BQ: QueryJob (parametrized SQL)
    BQ-->>FA: RowIterator
    FA->>FA: RowIterator â†’ Itemå¤‰æ›ï¼ˆè¾æ›¸å±•é–‹ï¼‰
    FA-->>C: JSON [{"id": ..., "name": ...}]
```

---

## ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install fastapi uvicorn google-cloud-bigquery pydantic
```

`pyproject.toml` ã§ç®¡ç†ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã—ã¾ã™ã€‚

```toml
[project]
dependencies = [
    "fastapi>=0.115.0",
    "uvicorn[standard]>=0.32.0",
    "google-cloud-bigquery>=3.25.0",
    "pydantic>=2.0.0",
]
```

### èªè¨¼è¨­å®š

ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºã§ã¯ ADCï¼ˆApplication Default Credentialsï¼‰ã‚’ä½¿ã„ã¾ã™ã€‚

```bash
gcloud auth application-default login
```

Cloud Run ãªã©ã®æœ¬ç•ªç’°å¢ƒã§ã¯ã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«é©åˆ‡ãªIAMãƒ­ãƒ¼ãƒ«ã‚’ä»˜ä¸ã—ã¾ã™ã€‚

```
roles/bigquery.dataEditor  # DMLæ“ä½œï¼ˆINSERT/UPDATE/DELETEï¼‰
roles/bigquery.jobUser     # ã‚¯ã‚¨ãƒªã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
```

### ç’°å¢ƒå¤‰æ•°

```bash
# .envï¼ˆGitã«å«ã‚ãªã„ã“ã¨ï¼‰
GCP_PROJECT_ID=your-project-id
BQ_DATASET=your_dataset
```

---

## BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä¾å­˜æ€§æ³¨å…¥ï¼ˆDependsï¼‰

FastAPI ã® `Depends()` ã‚’ä½¿ã£ã¦BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«æ³¨å…¥ã—ã¾ã™ã€‚ã“ã®è¨­è¨ˆã«ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚¹ãƒˆæ™‚ã«ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«å·®ã—æ›¿ãˆã‚‰ã‚Œã¾ã™ã€‚

```python
# dependencies.py
import os
from functools import lru_cache
from google.cloud import bigquery


@lru_cache(maxsize=1)
def get_settings() -> dict:
    """ç’°å¢ƒå¤‰æ•°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦è¿”ã™ã€‚"""
    return {
        "project_id": os.environ["GCP_PROJECT_ID"],
        "dataset": os.environ["BQ_DATASET"],
    }


def get_bq_client() -> bigquery.Client:
    """BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã¦è¿”ã™ã€‚

    BigQueryClientã¯ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ãªã®ã§ã€ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã¨ã—ã¦ç®¡ç†ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚
    ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«ç”Ÿæˆã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚
    """
    settings = get_settings()
    return bigquery.Client(project=settings["project_id"])
```

ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã®ä½¿ã„æ–¹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python
from fastapi import Depends
from google.cloud import bigquery
from dependencies import get_bq_client, get_settings

@app.get("/items")
async def list_items(client: bigquery.Client = Depends(get_bq_client)):
    settings = get_settings()
    # client ã¨ settings ã‚’ä½¿ã£ã¦ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    ...
```

---

## Pydanticãƒ¢ãƒ‡ãƒ«å®šç¾©ï¼ˆBQã‚¹ã‚­ãƒ¼ãƒã¨ã®å‹ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰

BigQueryã®å‹ã¨Pydantic v2ã®å‹ã¯æ¬¡ã®ã‚ˆã†ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚

| BigQueryå‹ | Pythonå‹ | Pydanticå‹ |
|-----------|---------|-----------|
| STRING | str | str |
| INT64 / INTEGER | int | int |
| FLOAT64 / FLOAT | float | float |
| BOOL / BOOLEAN | bool | bool |
| TIMESTAMP | datetime | datetime |
| DATE | date | date |
| RECORD / STRUCT | dict | BaseModelï¼ˆãƒã‚¹ãƒˆï¼‰ |
| REPEATED STRING | list[str] | list[str] |

æœ¬è¨˜äº‹ã§ã¯ `items` ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ä½¿ã„ã¾ã™ã€‚ã¾ãšBigQueryã«ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚

```sql
CREATE TABLE IF NOT EXISTS `{project}.{dataset}.items` (
  id        STRING    NOT NULL,
  name      STRING    NOT NULL,
  description STRING,
  price     FLOAT64,
  category  STRING,
  is_deleted BOOLEAN  DEFAULT FALSE,
  created_at TIMESTAMP,
  updated_at TIMESTAMP
)
PARTITION BY DATE(created_at)
CLUSTER BY category;
```

ã“ã®ã‚¹ã‚­ãƒ¼ãƒã«å¯¾å¿œã™ã‚‹Pydanticãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚

```python
# models.py
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime
import uuid


class ItemBase(BaseModel):
    """ã‚¢ã‚¤ãƒ†ãƒ ã®å…±é€šãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã€‚"""
    name: str
    description: Optional[str] = None
    price: Optional[float] = None
    category: Optional[str] = None


class ItemCreate(ItemBase):
    """POST /items ã§å—ã‘å–ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ã€‚"""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))


class ItemUpdate(BaseModel):
    """PUT /items/{id} ã§å—ã‘å–ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£ï¼ˆå…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä»»æ„ï¼‰ã€‚"""
    name: Optional[str] = None
    description: Optional[str] = None
    price: Optional[float] = None
    category: Optional[str] = None


class Item(ItemBase):
    """GETãƒ¬ã‚¹ãƒãƒ³ã‚¹ã§è¿”ã™ã‚¢ã‚¤ãƒ†ãƒ ãƒ¢ãƒ‡ãƒ«ã€‚"""
    id: str
    is_deleted: bool = False
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None

    model_config = {"from_attributes": True}
```

---

## GETã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

### ä¸€è¦§å–å¾—ï¼ˆãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãï¼‰

```python
# main.py
from fastapi import FastAPI, Depends, HTTPException, Query
from google.cloud import bigquery
from typing import Optional
import os

from models import Item
from dependencies import get_bq_client, get_settings

app = FastAPI(title="FastAPI Ã— BigQuery CRUD")


@app.get("/items", response_model=list[Item])
async def list_items(
    limit: int = Query(default=100, ge=1, le=1000),
    offset: int = Query(default=0, ge=0),
    category: Optional[str] = Query(default=None),
    client: bigquery.Client = Depends(get_bq_client),
):
    """ã‚¢ã‚¤ãƒ†ãƒ ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚

    - limit: 1ã€œ1000ä»¶ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100ï¼‰
    - offset: ã‚¹ã‚­ãƒƒãƒ—ä»¶æ•°
    - category: ã‚«ãƒ†ã‚´ãƒªãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä»»æ„ï¼‰
    """
    settings = get_settings()
    table = f"`{settings['project_id']}.{settings['dataset']}.items`"

    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚¯ã‚¨ãƒªã§SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã‚’é˜²ã
    query_params = [
        bigquery.ScalarQueryParameter("limit", "INT64", limit),
        bigquery.ScalarQueryParameter("offset", "INT64", offset),
    ]

    where_clause = "WHERE is_deleted = FALSE"
    if category is not None:
        where_clause += " AND category = @category"
        query_params.append(
            bigquery.ScalarQueryParameter("category", "STRING", category)
        )

    query = f"""
        SELECT id, name, description, price, category,
               is_deleted, created_at, updated_at
        FROM {table}
        {where_clause}
        ORDER BY created_at DESC
        LIMIT @limit OFFSET @offset
    """

    job_config = bigquery.QueryJobConfig(query_parameters=query_params)

    try:
        rows = client.query(query, job_config=job_config).result()
        return [Item(**dict(row)) for row in rows]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"BigQuery error: {str(e)}")
```

### å˜ä»¶å–å¾—

```python
@app.get("/items/{item_id}", response_model=Item)
async def get_item(
    item_id: str,
    client: bigquery.Client = Depends(get_bq_client),
):
    """æŒ‡å®šIDã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚"""
    settings = get_settings()
    table = f"`{settings['project_id']}.{settings['dataset']}.items`"

    query = f"""
        SELECT id, name, description, price, category,
               is_deleted, created_at, updated_at
        FROM {table}
        WHERE id = @item_id AND is_deleted = FALSE
        LIMIT 1
    """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("item_id", "STRING", item_id),
        ]
    )

    try:
        rows = list(client.query(query, job_config=job_config).result())
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"BigQuery error: {str(e)}")

    if not rows:
        raise HTTPException(status_code=404, detail=f"Item '{item_id}' not found")

    return Item(**dict(rows[0]))
```

---

## POSTã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆINSERTãƒ»å†ªç­‰MERGEï¼‰

BigQueryã§INSERTã‚’å®Ÿè£…ã™ã‚‹éš›ã¯ã€ **åŒã˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒ2å›æ¥ã¦ã‚‚é‡è¤‡ã—ãªã„å†ªç­‰ãªMERGEæ–‡** ã‚’ä½¿ã†ã“ã¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚é€šå¸¸ã®INSERTã¯å†è©¦è¡Œã«ã‚ˆã£ã¦é‡è¤‡è¡ŒãŒç™ºç”Ÿã™ã‚‹ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚

### å†ªç­‰INSERTï¼ˆMERGEæ–‡ã‚’ä½¿ã£ãŸ upsertï¼‰

```python
from datetime import datetime, timezone
from models import ItemCreate


@app.post("/items", response_model=Item, status_code=201)
async def create_item(
    body: ItemCreate,
    client: bigquery.Client = Depends(get_bq_client),
):
    """ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä½œæˆã—ã¾ã™ã€‚

    åŒã˜IDã§å†ãƒªã‚¯ã‚¨ã‚¹ãƒˆã•ã‚ŒãŸå ´åˆã¯æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã¾ã™ï¼ˆå†ªç­‰ï¼‰ã€‚
    """
    settings = get_settings()
    table = f"`{settings['project_id']}.{settings['dataset']}.items`"
    now = datetime.now(timezone.utc).isoformat()

    # MERGEæ–‡ã§å†ªç­‰INSERTï¼ˆupsertï¼‰
    query = f"""
        MERGE {table} AS target
        USING (
            SELECT
                @id AS id,
                @name AS name,
                @description AS description,
                @price AS price,
                @category AS category,
                FALSE AS is_deleted,
                CAST(@created_at AS TIMESTAMP) AS created_at,
                CAST(@updated_at AS TIMESTAMP) AS updated_at
        ) AS source
        ON target.id = source.id
        WHEN MATCHED THEN
            UPDATE SET
                name        = source.name,
                description = source.description,
                price       = source.price,
                category    = source.category,
                updated_at  = source.updated_at
        WHEN NOT MATCHED THEN
            INSERT (id, name, description, price, category,
                    is_deleted, created_at, updated_at)
            VALUES (source.id, source.name, source.description,
                    source.price, source.category, source.is_deleted,
                    source.created_at, source.updated_at)
    """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("id", "STRING", body.id),
            bigquery.ScalarQueryParameter("name", "STRING", body.name),
            bigquery.ScalarQueryParameter(
                "description", "STRING", body.description
            ),
            bigquery.ScalarQueryParameter("price", "FLOAT64", body.price),
            bigquery.ScalarQueryParameter("category", "STRING", body.category),
            bigquery.ScalarQueryParameter("created_at", "STRING", now),
            bigquery.ScalarQueryParameter("updated_at", "STRING", now),
        ]
    )

    try:
        client.query(query, job_config=job_config).result()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"BigQuery error: {str(e)}")

    # ä½œæˆã—ãŸãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
    return await get_item(body.id, client=client)
```

:::message

**ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIã¯é¿ã‘ã‚‹ã“ã¨**

BigQueryã«ã¯ `insert_rows_json()` ã¨ã„ã†ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIãŒã‚ã‚Šã¾ã™ãŒã€æœ¬ç•ªã§ã®é€šå¸¸CRUDç”¨é€”ã«ã¯æ¨å¥¨ã—ã¾ã›ã‚“ã€‚ç†ç”±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

- æ›¸ãè¾¼ã¿å¾Œ90åˆ†é–“ã¯DMLã§æ›´æ–°ãƒ»å‰Šé™¤ã§ããªã„ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒƒãƒ•ã‚¡æœŸé–“ï¼‰
- é‡è¤‡é™¤å»ã®ä¿è¨¼ãŒãªã„
- ã‚³ã‚¹ãƒˆãŒé«˜ã‚ï¼ˆ1MBã‚ãŸã‚Š$0.01ï¼‰

é€šå¸¸ã®DMLï¼ˆMERGEæ–‡ï¼‰ã‚’ä½¿ã†ã»ã†ãŒå®‰å…¨ã§ã™ã€‚
:::

---

## PUTã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆUPDATEãƒ»MERGEæ–‡ï¼‰

```python
from models import ItemUpdate


@app.put("/items/{item_id}", response_model=Item)
async def update_item(
    item_id: str,
    body: ItemUpdate,
    client: bigquery.Client = Depends(get_bq_client),
):
    """æŒ‡å®šIDã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ›´æ–°ã—ã¾ã™ã€‚

    å­˜åœ¨ã—ãªã„IDã‚’æŒ‡å®šã—ãŸå ´åˆã¯404ã‚’è¿”ã—ã¾ã™ã€‚
    """
    settings = get_settings()
    table = f"`{settings['project_id']}.{settings['dataset']}.items`"

    # æ›´æ–°å‰ã«å­˜åœ¨ç¢ºèª
    existing = await get_item(item_id, client=client)  # å­˜åœ¨ã—ãªã‘ã‚Œã°404

    now = datetime.now(timezone.utc).isoformat()

    # æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿UPDATE
    update_fields = []
    query_params = [
        bigquery.ScalarQueryParameter("item_id", "STRING", item_id),
        bigquery.ScalarQueryParameter("updated_at", "STRING", now),
    ]

    if body.name is not None:
        update_fields.append("name = @name")
        query_params.append(
            bigquery.ScalarQueryParameter("name", "STRING", body.name)
        )
    if body.description is not None:
        update_fields.append("description = @description")
        query_params.append(
            bigquery.ScalarQueryParameter(
                "description", "STRING", body.description
            )
        )
    if body.price is not None:
        update_fields.append("price = @price")
        query_params.append(
            bigquery.ScalarQueryParameter("price", "FLOAT64", body.price)
        )
    if body.category is not None:
        update_fields.append("category = @category")
        query_params.append(
            bigquery.ScalarQueryParameter("category", "STRING", body.category)
        )

    if not update_fields:
        # æ›´æ–°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒä½•ã‚‚ãªã‘ã‚Œã°ç¾çŠ¶ã‚’ãã®ã¾ã¾è¿”ã™
        return existing

    update_fields.append("updated_at = CAST(@updated_at AS TIMESTAMP)")
    set_clause = ", ".join(update_fields)

    query = f"""
        UPDATE {table}
        SET {set_clause}
        WHERE id = @item_id AND is_deleted = FALSE
    """

    job_config = bigquery.QueryJobConfig(query_parameters=query_params)

    try:
        client.query(query, job_config=job_config).result()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"BigQuery error: {str(e)}")

    return await get_item(item_id, client=client)
```

---

## DELETEã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

BigQueryã§ã¯ç‰©ç†å‰Šé™¤ã‚ˆã‚Šã‚‚ **è«–ç†å‰Šé™¤** ï¼ˆ`is_deleted = TRUE`ï¼‰ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚ç†ç”±ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã§ã¯ç‰©ç†DELETEå¾Œã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è§£æ”¾ã«æ™‚é–“ãŒã‹ã‹ã‚‹
- èª¤å‰Šé™¤æ™‚ã®ãƒªã‚«ãƒãƒªãƒ¼ãŒå®¹æ˜“
- å‰Šé™¤å±¥æ­´ã‚’åˆ†æã‚¯ã‚¨ãƒªã§å‚ç…§ã§ãã‚‹

```python
@app.delete("/items/{item_id}", status_code=204)
async def delete_item(
    item_id: str,
    client: bigquery.Client = Depends(get_bq_client),
):
    """æŒ‡å®šIDã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’è«–ç†å‰Šé™¤ã—ã¾ã™ã€‚

    å­˜åœ¨ã—ãªã„IDã‚’æŒ‡å®šã—ãŸå ´åˆã¯404ã‚’è¿”ã—ã¾ã™ã€‚
    """
    settings = get_settings()
    table = f"`{settings['project_id']}.{settings['dataset']}.items`"

    # å­˜åœ¨ç¢ºèª
    await get_item(item_id, client=client)  # å­˜åœ¨ã—ãªã‘ã‚Œã°404

    now = datetime.now(timezone.utc).isoformat()

    query = f"""
        UPDATE {table}
        SET is_deleted = TRUE,
            updated_at = CAST(@updated_at AS TIMESTAMP)
        WHERE id = @item_id
    """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("item_id", "STRING", item_id),
            bigquery.ScalarQueryParameter("updated_at", "STRING", now),
        ]
    )

    try:
        client.query(query, job_config=job_config).result()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"BigQuery error: {str(e)}")

    # 204 No Content
    return None
```

---

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒé€å‡ºã™ã‚‹ä¾‹å¤–ã‚’ FastAPI ã® `HTTPException` ã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ã‚’ç”¨æ„ã—ã¦ãŠãã¨ä¾¿åˆ©ã§ã™ã€‚

```python
# error_handler.py
from fastapi import HTTPException
from google.api_core.exceptions import (
    NotFound,
    BadRequest,
    Forbidden,
    GoogleAPICallError,
)
import logging

logger = logging.getLogger(__name__)


def handle_bq_exception(e: Exception, context: str = "") -> None:
    """BigQueryä¾‹å¤–ã‚’HTTPExceptionã«å¤‰æ›ã—ã¦é€å‡ºã™ã‚‹ã€‚

    Args:
        e: ç™ºç”Ÿã—ãŸä¾‹å¤–
        context: ã‚¨ãƒ©ãƒ¼ã®æ–‡è„ˆï¼ˆãƒ­ã‚°ç”¨ï¼‰
    """
    if isinstance(e, NotFound):
        logger.warning(f"BigQuery NotFound [{context}]: {e}")
        raise HTTPException(status_code=404, detail=str(e))

    if isinstance(e, BadRequest):
        logger.error(f"BigQuery BadRequest [{context}]: {e}")
        raise HTTPException(
            status_code=400, detail=f"Invalid query: {str(e)}"
        )

    if isinstance(e, Forbidden):
        logger.error(f"BigQuery Forbidden [{context}]: {e}")
        raise HTTPException(
            status_code=403,
            detail="BigQuery access denied. Check IAM permissions.",
        )

    if isinstance(e, GoogleAPICallError):
        logger.error(f"BigQuery API error [{context}]: {e}")
        raise HTTPException(
            status_code=503,
            detail="BigQuery is temporarily unavailable.",
        )

    logger.error(f"Unexpected error [{context}]: {e}", exc_info=True)
    raise HTTPException(status_code=500, detail="Internal server error")
```

ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã®ä½¿ç”¨ä¾‹ã§ã™ã€‚

```python
from error_handler import handle_bq_exception

@app.get("/items/{item_id}", response_model=Item)
async def get_item(
    item_id: str,
    client: bigquery.Client = Depends(get_bq_client),
):
    settings = get_settings()
    table = f"`{settings['project_id']}.{settings['dataset']}.items`"

    query = f"""
        SELECT id, name, description, price, category,
               is_deleted, created_at, updated_at
        FROM {table}
        WHERE id = @item_id AND is_deleted = FALSE
        LIMIT 1
    """

    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("item_id", "STRING", item_id),
        ]
    )

    try:
        rows = list(client.query(query, job_config=job_config).result())
    except Exception as e:
        handle_bq_exception(e, context=f"get_item({item_id})")

    if not rows:
        raise HTTPException(status_code=404, detail=f"Item '{item_id}' not found")

    return Item(**dict(rows[0]))
```

---

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€FastAPI Ã— BigQuery ã§CRUD APIã‚’å®Ÿè£…ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚

### å®Ÿè£…ã®ãƒã‚¤ãƒ³ãƒˆæ•´ç†

| ãƒã‚¤ãƒ³ãƒˆ | å¯¾ç­– |
|---------|------|
| SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­– | `bigquery.ScalarQueryParameter` ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ– |
| å†ªç­‰ãªæ›¸ãè¾¼ã¿ | MERGEæ–‡ã§ upsertï¼ˆå†è©¦è¡Œã—ã¦ã‚‚é‡è¤‡ã—ãªã„ï¼‰ |
| è«–ç†å‰Šé™¤ | `is_deleted = TRUE` ã§å‰Šé™¤ã€‚ç‰©ç†DELETEã¯æœ€çµ‚æ‰‹æ®µ |
| ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç† | `Depends()` ã§æ³¨å…¥ã€ãƒ†ã‚¹ãƒˆæ™‚ã«ãƒ¢ãƒƒã‚¯å·®ã—æ›¿ãˆå¯èƒ½ |
| ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° | BigQueryä¾‹å¤– â†’ HTTPException ã¸ã®å¤‰æ›ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ |

### æ³¨æ„äº‹é …

- **f-string ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŸ‹ã‚è¾¼ã¿ã¯ç¦æ­¢**: SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™
- **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIï¼ˆ`insert_rows_json`ï¼‰ã¯é€šå¸¸CRUDã«ã¯ä¸å‘ã**: DMLåˆ¶é™ï¼ˆ90åˆ†ãƒãƒƒãƒ•ã‚¡ï¼‰ãŒã‚ã‚Šã¾ã™
- **DMLã‚¯ã‚©ãƒ¼ã‚¿ã«æ³¨æ„**: 1ãƒ†ãƒ¼ãƒ–ãƒ«ã‚ãŸã‚Š1æ—¥1,500å›ã®DMLã‚¯ã‚©ãƒ¼ã‚¿ãŒã‚ã‚‹ãŸã‚ã€é«˜é »åº¦ã®æ›¸ãè¾¼ã¿ã«ã¯å‘ãã¾ã›ã‚“
- **ã‚³ã‚¹ãƒˆç®¡ç†**: `SELECT *` ã¯é¿ã‘ã€å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿ã‚’æŒ‡å®šã—ã¦ã‚¹ã‚­ãƒ£ãƒ³é‡ã‚’æŠ‘ãˆã¾ã™

### å‘ã„ã¦ã„ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ãƒ»å‘ã„ã¦ã„ãªã„ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

**å‘ã„ã¦ã„ã‚‹**
- åˆ†æçµæœã®é–²è¦§APIã‚„ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
- æ›¸ãè¾¼ã¿ã‚ˆã‚Šèª­ã¿å–ã‚ŠãŒå¤šã„æ§‹æˆ
- BigQueryã‚’Single Source of Truthã¨ã—ãŸãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ 

**å‘ã„ã¦ã„ãªã„**
- æ¯ç§’æ•°åƒä»¶ã®æ›¸ãè¾¼ã¿ãŒç™ºç”Ÿã™ã‚‹ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ API
- ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³æ•´åˆæ€§ãŒå¿…è¦ãªECã‚„é‡‘èç³»
- ãƒŸãƒªç§’ä»¥ä¸‹ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒè¦æ±‚ã•ã‚Œã‚‹ç”¨é€”

BigQueryã¯ã€Œãªã‚“ã§ã‚‚ã§ãã‚‹RDBã€ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€GCPã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®ä¸­ã§åˆ†æã¨APIã‚’çµ±åˆã—ãŸã„å ´é¢ã§ã¯éå¸¸ã«å¼·åŠ›ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ãªã‚Šã¾ã™ã€‚æœ¬è¨˜äº‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‚è€ƒã«ã€ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«åˆã‚ã›ã¦æ¡ç”¨ã‚’æ¤œè¨ã—ã¦ã¿ã¦ãã ã•ã„ã€‚
