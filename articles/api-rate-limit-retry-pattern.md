---
title:" "APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ â€” è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ã®å®Ÿè£…""
emoji: "ğŸ”„"
type: "tech"
topics: ["api", "githubactions", "python", "automation", "errorhandling"]
published: true
publication_name: "correlate_dev"
---

## ã¯ã˜ã‚ã« â€” APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã¯èª°ã‚‚ãŒç›´é¢ã™ã‚‹å•é¡Œ

APIã‚’åˆ©ç”¨ã—ãŸè‡ªå‹•åŒ–ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã¨ã€å¿…ãšç›´é¢ã™ã‚‹ã®ãŒ**ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆï¼ˆRate Limitï¼‰**ã§ã™ã€‚

ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã¨ã¯ã€ä¸€å®šæ™‚é–“å†…ã«å®Ÿè¡Œã§ãã‚‹APIãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã®ä¸Šé™ã®ã“ã¨ã§ã™ã€‚ã‚µãƒ¼ãƒ“ã‚¹æä¾›å´ãŒã‚µãƒ¼ãƒãƒ¼è² è·ã‚’ç®¡ç†ã—ã€å…¬å¹³ãªãƒªã‚½ãƒ¼ã‚¹åˆ†é…ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚

### ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã®ç¨®é¡

ä¸»ãªãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã®ç¨®é¡ã¯ä»¥ä¸‹ã®3ã¤ã§ã™ã€‚

| ç¨®é¡ | èª¬æ˜ | ä¾‹ |
|------|------|-----|
| **æ™‚é–“ã‚ãŸã‚Šåˆ¶é™** | 1æ™‚é–“ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ä¸Šé™ | GitHub API: 5,000/æ™‚ï¼ˆèªè¨¼æ¸ˆã¿ï¼‰ |
| **æ—¥ã‚ãŸã‚Šåˆ¶é™** | 1æ—¥ã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ä¸Šé™ | Twitter API: 50ãƒ„ã‚¤ãƒ¼ãƒˆ/æ—¥ï¼ˆFree Tierï¼‰ |
| **ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦** | ç›´è¿‘Næ™‚é–“ã®ç´¯ç©ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° | Zenn: ç›´è¿‘24æ™‚é–“ã§5æœ¬ã¾ã§æŠ•ç¨¿å¯èƒ½ |

### ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆè¶…éæ™‚ã®å½±éŸ¿

ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã‚’è¶…ãˆã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ã€‚

- **å‡¦ç†ã®ä¸­æ–­**: APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒ `429 Too Many Requests` ã‚¨ãƒ©ãƒ¼ã§æ‹’å¦ã•ã‚Œã‚‹
- **ãƒ‡ãƒ¼ã‚¿æå¤±**: æŠ•ç¨¿äºˆå®šã ã£ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå…¬é–‹ã•ã‚Œãªã„
- **ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“ã®æ‚ªåŒ–**: Botå¿œç­”é…å»¶ã€é€šçŸ¥ã®é…ã‚Œ
- **ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåœæ­¢ãƒªã‚¹ã‚¯**: é »ç¹ãªè¶…éã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆBANã®å¯èƒ½æ€§

### ã“ã®è¨˜äº‹ã§å­¦ã¹ã‚‹ã“ã¨

æœ¬è¨˜äº‹ã§ã¯ã€å®Ÿéš›ã«ç§ãŒé­é‡ã—ãŸZennè‡ªå‹•æŠ•ç¨¿ã®å¤±æ•—äº‹ä¾‹ã‚’å…ƒã«ã€**4ã‚¹ãƒ†ãƒƒãƒ—ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³**ã§è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ã‚’å®Ÿè£…ã™ã‚‹æ–¹æ³•ã‚’è§£èª¬ã—ã¾ã™ã€‚

- âœ… ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¤±æ•—ã‚’è‡ªå‹•æ¤œçŸ¥ã™ã‚‹ä»•çµ„ã¿
- âœ… å¤±æ•—ã—ãŸå‡¦ç†ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹æ–¹æ³•
- âœ… ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³
- âœ… æ—¢å­˜ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ç«¶åˆã—ãªã„ãƒªãƒˆãƒ©ã‚¤ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 
- âœ… GitHub Actionsã§ã®è‡ªå‹•åŒ–å®Ÿè£…

å®Ÿè£…ä¾‹ã¯Pythonã§ã™ãŒã€è€ƒãˆæ–¹ã¯ã©ã®è¨€èªãƒ»ã©ã®APIã«ã‚‚å¿œç”¨ã§ãã¾ã™ã€‚

---

## å®Ÿä¾‹ï¼šZennã§8æœ¬ã®è¨˜äº‹ãŒæ¶ˆãˆãŸæ—¥

### ä½•ãŒèµ·ããŸã®ã‹

2026å¹´2æœˆ14æ—¥ã€ç§ã¯8æœ¬ã®Zennè¨˜äº‹ã‚’äºˆç´„å…¬é–‹ã—ã¾ã—ãŸã€‚GitHub Actionsã®cronã§è‡ªå‹•æŠ•ç¨¿ã™ã‚‹è¨­å®šã§ã€`published_at`ã«åŒã˜æ™‚åˆ»ã‚’è¨­å®šã—ã¦ã„ã¾ã—ãŸã€‚

ã—ã‹ã—ã€**8æœ¬ã™ã¹ã¦ãŒZennã«å…¬é–‹ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ**ã€‚

### å•é¡Œã®æ·±åˆ»ã•

ãƒ­ãƒ¼ã‚«ãƒ«ã®Markdownãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã—ãŸã€‚

```yaml
published: true
```

GitHub Actionsä¸Šã§ã‚‚ã€Œãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸã€ã¨è¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã—ãŸãŒã€å®Ÿéš›ã«Zennä¸Šã§è¨˜äº‹ã‚’ç¢ºèªã™ã‚‹ã¨**404 Not Found**ã§ã—ãŸã€‚

ã¤ã¾ã‚Šã€**ãƒ­ãƒ¼ã‚«ãƒ«ã¨GitHubã§ã¯å…¬é–‹æ¸ˆã¿ã€Zennä¸Šã§ã¯æœªå…¬é–‹**ã¨ã„ã†ä¸æ•´åˆçŠ¶æ…‹ã«é™¥ã‚Šã¾ã—ãŸã€‚

### åŸå› ï¼š24æ™‚é–“ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ç½ 

Zennã®ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆä»•æ§˜ã‚’èª¿æŸ»ã—ãŸã¨ã“ã‚ã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚

> Zennã§ã¯ä¸€å®šæ™‚é–“ã‚ãŸã‚Šã®æŠ•ç¨¿æ•°ã«ä¸Šé™ãŒã‚ã‚Šã¾ã™ã€‚ç›´è¿‘24æ™‚é–“ä»¥å†…ã«5æœ¬ä»¥ä¸ŠæŠ•ç¨¿ã™ã‚‹ã¨ã€ãã‚Œä»¥é™ã®æŠ•ç¨¿ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã™ã€‚
>
> â€” [Zenn FAQ - ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆ](https://zenn.dev/faq)

ç§ã¯8æœ¬ã™ã¹ã¦ã«åŒã˜`published_at`ã‚’è¨­å®šã—ã¦ã„ãŸãŸã‚ã€**8æœ¬ãŒåŒæ™‚ã«ãƒ‡ãƒ—ãƒ­ã‚¤å¯¾è±¡ã«ãªã‚Šã€ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã§å…¨ä»¶ãƒ–ãƒ­ãƒƒã‚¯**ã•ã‚ŒãŸã®ã§ã™ã€‚

ã•ã‚‰ã«å•é¡Œã ã£ãŸã®ã¯ã€**ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ­ã‚°ã«ä½•ã®ã‚¨ãƒ©ãƒ¼ã‚‚å‡ºåŠ›ã•ã‚Œãªã‹ã£ãŸ**ã“ã¨ã§ã™ã€‚

```
æ¬¡ã®è¨˜äº‹ã¯æŠ•ç¨¿æ•°ã®ä¸Šé™ã«é”ã—ãŸãŸã‚ãƒ‡ãƒ—ãƒ­ã‚¤ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ:
article-1, article-2, article-3, article-4, article-5, article-6, article-7, article-8
```

ã“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒå‡ºãŸã®ã¯ãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã®ãƒ­ã‚°æœ«å°¾ã§ã€GitHub Actionsã®å®Ÿè¡Œçµæœã¯**æˆåŠŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹**ã®ã¾ã¾ã§ã—ãŸã€‚

### å¤±æ•—ã‚’æ”¾ç½®ã™ã‚‹ã¨ã©ã†ãªã‚‹ã‹

ã“ã®çŠ¶æ…‹ã‚’æ”¾ç½®ã™ã‚‹ã¨ã€æ¬¡ã®ã‚ˆã†ãªå•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ã€‚

1. **ãƒ­ãƒ¼ã‚«ãƒ«ã¨Zennã®çŠ¶æ…‹ä¸æ•´åˆ**
   - ãƒ­ãƒ¼ã‚«ãƒ«: `published: true`
   - Zenn: æœªå…¬é–‹ï¼ˆ404ï¼‰

2. **å†å®Ÿè¡Œã§ããªã„**
   - `published: true`ã®ã¾ã¾ãªã®ã§ã€å†åº¦ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦ã‚‚ä½•ã‚‚èµ·ããªã„

3. **æ‰‹å‹•ä¿®æ­£ãŒå¿…è¦**
   - 8æœ¬ã™ã¹ã¦ã‚’æ‰‹å‹•ã§`published: false`ã«æˆ»ã™
   - `published_at`ã‚’å†è¨­å®šã™ã‚‹
   - Git commitã—ã¦å†ãƒ‡ãƒ—ãƒ­ã‚¤

ã“ã®æ‰‹å‹•ä½œæ¥­ã¯**ã‚¨ãƒ©ãƒ¼ã®æ¸©åºŠ**ã§ã‚ã‚Šã€ç‰¹ã«è¨˜äº‹æ•°ãŒå¤šã„å ´åˆã¯è¦‹è½ã¨ã—ãŒç™ºç”Ÿã—ã¾ã™ã€‚

---

## ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–ã®4ã‚¹ãƒ†ãƒƒãƒ—è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³

ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¤±æ•—ã‚’è‡ªå‹•ã§å›å¾©ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®4ã‚¹ãƒ†ãƒƒãƒ—ãŒå¿…è¦ã§ã™ã€‚

```mermaid
graph TD
    A[1. æ¤œè¨¼ Verification] --> B[2. ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ Rollback]
    B --> C[3. ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚° Queueing]
    C --> D[4. ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° Scheduling]
    D --> E[ãƒªãƒˆãƒ©ã‚¤å®Ÿè¡Œ]
```

ãã‚Œãã‚Œã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’è©³ã—ãè¦‹ã¦ã„ãã¾ã™ã€‚

### 3.1 æ¤œè¨¼ï¼ˆVerificationï¼‰

**ç›®çš„**: APIãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå®Ÿéš›ã«æˆåŠŸã—ãŸã‹ã‚’ç¢ºèªã™ã‚‹

GitHub Actionsã‚„ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ„ãƒ¼ãƒ«ãŒã€ŒæˆåŠŸã€ã¨å ±å‘Šã—ã¦ã‚‚ã€å®Ÿéš›ã«APIå´ã§å‡¦ç†ãŒå®Œäº†ã—ã¦ã„ã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“ã€‚

**æ¤œè¨¼æ–¹æ³•**:
1. APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ç¢ºèª
2. HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ï¼ˆ200/404ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
3. ãƒ­ãƒ¼ã‚«ãƒ«ã®çŠ¶æ…‹ã¨æ¯”è¼ƒ

**å®Ÿè£…ä¾‹**:

```python
import requests
from typing import Optional

def check_published_on_zenn(slug: str, username: str = "correlate") -> bool:
    """Zennã§å®Ÿéš›ã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    url = f"https://zenn.dev/{username}/articles/{slug}"
    try:
        response = requests.head(url, timeout=10, allow_redirects=True)
        return response.status_code in [200, 301]
    except requests.RequestException:
        return False
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- `HEAD`ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§è»½é‡ã«ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒœãƒ‡ã‚£ä¸è¦ï¼‰
- ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®šã§ç„¡é™å¾…ã¡ã‚’é˜²ã
- ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆï¼ˆ301ï¼‰ã‚‚æˆåŠŸæ‰±ã„

### 3.2 ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆRollbackï¼‰

**ç›®çš„**: å¤±æ•—ã—ãŸçŠ¶æ…‹ã‚’å…ƒã«æˆ»ã™

æ¤œè¨¼ã§å¤±æ•—ãŒç¢ºèªã•ã‚ŒãŸå ´åˆã€ãƒ­ãƒ¼ã‚«ãƒ«ã®çŠ¶æ…‹ã‚’ã€Œæœªå…¬é–‹ã€ã«æˆ»ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ¬¡å›ã®ãƒ‡ãƒ—ãƒ­ã‚¤ã§å†åº¦å‡¦ç†å¯¾è±¡ã«ãªã‚Šã¾ã™ã€‚

**å®Ÿè£…ä¾‹**:

```python
import re
from pathlib import Path

def rollback_published_flag(file_path: Path):
    """published: true â†’ false ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    content = file_path.read_text(encoding="utf-8")

    # published: true â†’ false ã«å¤‰æ›´
    updated = re.sub(
        r'^published:\s*true',
        'published: false',
        content,
        flags=re.MULTILINE
    )

    # published_at ã‚‚å‰Šé™¤ï¼ˆZennã®ä»•æ§˜: published: false + published_at ã¯ä¸æ­£ï¼‰
    updated = re.sub(
        r'^published_at:.*\n',
        '',
        updated,
        flags=re.MULTILINE
    )

    file_path.write_text(updated, encoding="utf-8")
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- `published: true` â†’ `published: false` ã ã‘ã§ãªãã€`published_at`ã‚‚å‰Šé™¤
- Zennã®ä»•æ§˜ã§ã¯ã€`published: false` + `published_at`ã®çµ„ã¿åˆã‚ã›ã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹
- æ­£è¦è¡¨ç¾ã§ç¢ºå®Ÿã«ç½®æ›

**æ³¨æ„**: ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å¾Œã¯å¿…ãšGit commitãŒå¿…è¦ã§ã™ã€‚

```python
import subprocess

def commit_rollback(files: list[Path], message: str = "Rollback failed articles"):
    """ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã‚³ãƒŸãƒƒãƒˆ"""
    subprocess.run(["git", "add"] + [str(f) for f in files], check=True)
    subprocess.run(["git", "commit", "-m", message], check=True)
    subprocess.run(["git", "push"], check=True)
```

### 3.3 ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°ï¼ˆQueueingï¼‰

**ç›®çš„**: å¤±æ•—ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’è¨˜éŒ²ã—ã€ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã¨ã—ã¦ç®¡ç†ã™ã‚‹

å˜ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹ã ã‘ã§ã¯ã€æ¬¡å›ãƒ‡ãƒ—ãƒ­ã‚¤ã§ã¾ãŸåŒã˜ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã«è¨˜éŒ²ã™ã‚‹ã“ã¨ã§ã€**é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å†å®Ÿè¡Œ**ã§ãã¾ã™ã€‚

**ã‚­ãƒ¥ãƒ¼è¨­è¨ˆ**:

```json
{
  "queue": [
    {
      "slug": "article-1",
      "file_path": "articles/article-1.md",
      "failed_at": "2026-02-14T08:00:00Z",
      "retry_count": 0,
      "reason": "rate_limit"
    },
    {
      "slug": "article-2",
      "file_path": "articles/article-2.md",
      "failed_at": "2026-02-14T08:00:00Z",
      "retry_count": 0,
      "reason": "rate_limit"
    }
  ]
}
```

**å®Ÿè£…ä¾‹**:

```python
import json
from datetime import datetime
from pathlib import Path

QUEUE_FILE = Path(".zenn-retry-queue.json")

def add_to_retry_queue(slug: str, file_path: Path, reason: str = "rate_limit"):
    """å¤±æ•—ã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
    queue_data = load_queue()

    # æ—¢ã«ã‚­ãƒ¥ãƒ¼ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
    if any(item["slug"] == slug for item in queue_data["queue"]):
        return

    queue_data["queue"].append({
        "slug": slug,
        "file_path": str(file_path),
        "failed_at": datetime.utcnow().isoformat() + "Z",
        "retry_count": 0,
        "reason": reason,
    })

    save_queue(queue_data)

def load_queue() -> dict:
    """ã‚­ãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    if not QUEUE_FILE.exists():
        return {"queue": []}
    return json.loads(QUEUE_FILE.read_text(encoding="utf-8"))

def save_queue(data: dict):
    """ã‚­ãƒ¥ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
    QUEUE_FILE.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- JSONãƒ•ã‚¡ã‚¤ãƒ«ã§æ°¸ç¶šåŒ–ï¼ˆè»½é‡ã§ç®¡ç†ã—ã‚„ã™ã„ï¼‰
- `retry_count`ã§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¿½è·¡
- `reason`ã§ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥ã‚’è¨˜éŒ²ï¼ˆãƒ‡ãƒãƒƒã‚°ã«æœ‰ç”¨ï¼‰

### 3.4 ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆSchedulingï¼‰

**ç›®çš„**: ãƒªãƒˆãƒ©ã‚¤å®Ÿè¡Œæ™‚ã«æ—¢å­˜ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ç«¶åˆã—ãªã„ã‚ˆã†ã«ã™ã‚‹

ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–ã‚Šå‡ºã—ã¦å³åº§ã«å®Ÿè¡Œã™ã‚‹ã¨ã€**ã¾ãŸåŒã˜ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ**ã—ã¾ã™ã€‚

æ—¢å­˜ã®äºˆç´„æŠ•ç¨¿ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã€**ç©ºã„ã¦ã„ã‚‹ã‚¹ãƒ­ãƒƒãƒˆ**ã‚’è¦‹ã¤ã‘ã¦å†äºˆç´„ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **:

```python
import re
from datetime import datetime, timedelta
from pathlib import Path

ARTICLES_DIR = Path("articles")

def get_existing_scheduled_times() -> set:
    """æ—¢å­˜è¨˜äº‹ã®published_atã‚’å–å¾—ï¼ˆç«¶åˆãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
    scheduled_times = set()

    for article_file in ARTICLES_DIR.glob("*.md"):
        content = article_file.read_text(encoding="utf-8")
        match = re.search(r'^published_at:\s*["\']?([^"\']+)["\']?', content, re.MULTILINE)
        if match:
            scheduled_times.add(match.group(1).strip())

    return scheduled_times

def calculate_next_slots(count: int, start_time: datetime = None) -> list[str]:
    """æ¬¡ã®å…¬é–‹ã‚¹ãƒ­ãƒƒãƒˆã‚’è¨ˆç®—ï¼ˆæ—¢å­˜è¨˜äº‹ã¨ç«¶åˆã—ãªã„ã‚ˆã†ã«ï¼‰"""
    if start_time is None:
        start_time = datetime.now() + timedelta(hours=1)

    existing_times = get_existing_scheduled_times()
    slots = []
    current = start_time

    # 24æ™‚é–“ã«5æœ¬ãƒ«ãƒ¼ãƒ«ã‚’éµå®ˆ
    # 1æ—¥ã‚’3ã¤ã®ã‚¹ãƒ­ãƒƒãƒˆã«åˆ†å‰²: 08:00, 12:30, 19:00
    time_slots_per_day = [
        {"hour": 8, "minute": 0},
        {"hour": 12, "minute": 30},
        {"hour": 19, "minute": 0},
    ]

    while len(slots) < count:
        for slot_time in time_slots_per_day:
            candidate = current.replace(
                hour=slot_time["hour"],
                minute=slot_time["minute"],
                second=0,
                microsecond=0
            )

            # éå»ã®æ™‚åˆ»ã¯ã‚¹ã‚­ãƒƒãƒ—
            if candidate < datetime.now():
                continue

            slot_str = candidate.strftime("%Y-%m-%d %H:%M")

            # æ—¢å­˜ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨é‡è¤‡ã—ãªã„ã‹ãƒã‚§ãƒƒã‚¯
            if slot_str not in existing_times:
                slots.append(slot_str)

                if len(slots) >= count:
                    break

        # æ¬¡ã®æ—¥ã¸
        current += timedelta(days=1)

    return slots

def update_published_at(file_path: Path, published_at: str):
    """è¨˜äº‹ã®published_atã‚’æ›´æ–°"""
    content = file_path.read_text(encoding="utf-8")

    # published: false â†’ true
    content = re.sub(r'^published:\s*false', 'published: true', content, flags=re.MULTILINE)

    # published_at ã‚’è¿½åŠ ã¾ãŸã¯æ›´æ–°
    if "published_at:" in content:
        content = re.sub(
            r'^published_at:.*$',
            f'published_at: "{published_at}"',
            content,
            flags=re.MULTILINE
        )
    else:
        # front matterã®æœ«å°¾ã«è¿½åŠ 
        content = re.sub(
            r'^---$',
            f'published_at: "{published_at}"\n---',
            content,
            count=2,  # 2ç•ªç›®ã® --- ã«è¿½åŠ 
            flags=re.MULTILINE
        )

    file_path.write_text(content, encoding="utf-8")
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- æ—¢å­˜ã®å…¨è¨˜äº‹ã‚’ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦`published_at`ã‚’åé›†
- ç©ºãã‚¹ãƒ­ãƒƒãƒˆã®ã¿ã‚’é¸æŠ
- 24æ™‚é–“ã«5æœ¬ãƒ«ãƒ¼ãƒ«ã‚’éµå®ˆï¼ˆ1æ—¥3ã‚¹ãƒ­ãƒƒãƒˆ Ã— è¤‡æ•°æ—¥ï¼‰

---

## å®Ÿè£…ä¾‹ï¼šPython + GitHub Actions

ã“ã‚Œã¾ã§ã®4ã‚¹ãƒ†ãƒƒãƒ—ã‚’çµ±åˆã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å®Ÿè£…ã—ã¾ã™ã€‚

### 4.1 æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆzenn-verify-published.pyï¼‰

```python
#!/usr/bin/env python3
"""
Zennè¨˜äº‹ãŒå®Ÿéš›ã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‹æ¤œè¨¼ã—ã€å¤±æ•—ã—ãŸè¨˜äº‹ã‚’ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
"""
import sys
import re
import requests
from pathlib import Path
from typing import Optional
import json
from datetime import datetime

ARTICLES_DIR = Path("articles")
QUEUE_FILE = Path(".zenn-retry-queue.json")
USERNAME = "correlate"  # è‡ªåˆ†ã®Zennãƒ¦ãƒ¼ã‚¶ãƒ¼å

def check_published_on_zenn(slug: str) -> bool:
    """Zennã§å®Ÿéš›ã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    url = f"https://zenn.dev/{USERNAME}/articles/{slug}"
    try:
        response = requests.head(url, timeout=10, allow_redirects=True)
        return response.status_code in [200, 301]
    except requests.RequestException:
        return False

def get_published_articles() -> list[tuple[str, Path]]:
    """published: true ã®è¨˜äº‹ã‚’å–å¾—"""
    articles = []
    for article_file in ARTICLES_DIR.glob("*.md"):
        content = article_file.read_text(encoding="utf-8")
        if re.search(r'^published:\s*true', content, re.MULTILINE):
            slug = article_file.stem
            articles.append((slug, article_file))
    return articles

def rollback_published_flag(file_path: Path):
    """published: true â†’ false ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    content = file_path.read_text(encoding="utf-8")

    # published: true â†’ false
    updated = re.sub(r'^published:\s*true', 'published: false', content, flags=re.MULTILINE)

    # published_at ã‚‚å‰Šé™¤
    updated = re.sub(r'^published_at:.*\n', '', updated, flags=re.MULTILINE)

    file_path.write_text(updated, encoding="utf-8")

def add_to_retry_queue(slug: str, file_path: Path):
    """ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ """
    queue_data = {"queue": []} if not QUEUE_FILE.exists() else json.loads(QUEUE_FILE.read_text())

    if any(item["slug"] == slug for item in queue_data["queue"]):
        return  # æ—¢ã«å­˜åœ¨

    queue_data["queue"].append({
        "slug": slug,
        "file_path": str(file_path),
        "failed_at": datetime.utcnow().isoformat() + "Z",
        "retry_count": 0,
        "reason": "rate_limit",
    })

    QUEUE_FILE.write_text(json.dumps(queue_data, indent=2, ensure_ascii=False))

def main():
    articles = get_published_articles()
    failed_articles = []

    for slug, file_path in articles:
        if not check_published_on_zenn(slug):
            print(f"âŒ Failed: {slug}")
            failed_articles.append((slug, file_path))
        else:
            print(f"âœ… Published: {slug}")

    if failed_articles:
        print(f"\nğŸ”„ Rolling back {len(failed_articles)} failed articles...")
        for slug, file_path in failed_articles:
            rollback_published_flag(file_path)
            add_to_retry_queue(slug, file_path)
            print(f"  Rolled back: {slug}")

        # Git commit
        import subprocess
        subprocess.run(["git", "add", str(ARTICLES_DIR)], check=True)
        subprocess.run(["git", "add", str(QUEUE_FILE)], check=True)
        subprocess.run(["git", "commit", "-m", "Rollback failed articles"], check=True)
        subprocess.run(["git", "push"], check=True)
    else:
        print("\nâœ… All articles published successfully!")

if __name__ == "__main__":
    main()
```

### 4.2 ãƒªãƒˆãƒ©ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆzenn-retry-failed.pyï¼‰

```python
#!/usr/bin/env python3
"""
ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã‹ã‚‰è¨˜äº‹ã‚’å–ã‚Šå‡ºã—ã€ç©ºãã‚¹ãƒ­ãƒƒãƒˆã«å†äºˆç´„
"""
import sys
import re
import json
from pathlib import Path
from datetime import datetime, timedelta

ARTICLES_DIR = Path("articles")
QUEUE_FILE = Path(".zenn-retry-queue.json")
MAX_RETRY = 3  # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°

def get_existing_scheduled_times() -> set:
    """æ—¢å­˜è¨˜äº‹ã®published_atã‚’å–å¾—"""
    scheduled_times = set()
    for article_file in ARTICLES_DIR.glob("*.md"):
        content = article_file.read_text(encoding="utf-8")
        match = re.search(r'^published_at:\s*["\']?([^"\']+)["\']?', content, re.MULTILINE)
        if match:
            scheduled_times.add(match.group(1).strip())
    return scheduled_times

def calculate_next_slots(count: int) -> list[str]:
    """æ¬¡ã®å…¬é–‹ã‚¹ãƒ­ãƒƒãƒˆã‚’è¨ˆç®—"""
    existing_times = get_existing_scheduled_times()
    slots = []
    current = datetime.now() + timedelta(hours=1)

    time_slots_per_day = [
        {"hour": 8, "minute": 0},
        {"hour": 12, "minute": 30},
        {"hour": 19, "minute": 0},
    ]

    max_days = 14  # 2é€±é–“å…ˆã¾ã§æ¤œç´¢
    for day_offset in range(max_days):
        check_date = current + timedelta(days=day_offset)
        for slot_time in time_slots_per_day:
            candidate = check_date.replace(
                hour=slot_time["hour"],
                minute=slot_time["minute"],
                second=0,
                microsecond=0
            )

            if candidate < datetime.now():
                continue

            slot_str = candidate.strftime("%Y-%m-%d %H:%M")
            if slot_str not in existing_times:
                slots.append(slot_str)

                if len(slots) >= count:
                    return slots

    return slots

def update_published_at(file_path: Path, published_at: str):
    """è¨˜äº‹ã®published_atã‚’æ›´æ–°"""
    content = file_path.read_text(encoding="utf-8")
    content = re.sub(r'^published:\s*false', 'published: true', content, flags=re.MULTILINE)

    if "published_at:" in content:
        content = re.sub(
            r'^published_at:.*$',
            f'published_at: "{published_at}"',
            content,
            flags=re.MULTILINE
        )
    else:
        content = re.sub(r'^(---\n(?:.*\n)*?)---', rf'\1published_at: "{published_at}"\n---', content, count=1)

    file_path.write_text(content, encoding="utf-8")

def main():
    if not QUEUE_FILE.exists():
        print("No retry queue found.")
        return

    queue_data = json.loads(QUEUE_FILE.read_text())
    items = queue_data["queue"]

    if not items:
        print("Retry queue is empty.")
        return

    # ãƒªãƒˆãƒ©ã‚¤å›æ•°è¶…éã‚’é™¤å¤–
    valid_items = [item for item in items if item["retry_count"] < MAX_RETRY]

    if not valid_items:
        print("All items exceeded max retry count.")
        return

    # ã‚¹ãƒ­ãƒƒãƒˆã‚’è¨ˆç®—
    slots = calculate_next_slots(len(valid_items))

    if len(slots) < len(valid_items):
        print(f"âš ï¸  Only {len(slots)} slots available for {len(valid_items)} items.")

    # å†äºˆç´„
    updated_files = []
    for item, slot in zip(valid_items, slots):
        file_path = Path(item["file_path"])
        update_published_at(file_path, slot)
        updated_files.append(file_path)
        item["retry_count"] += 1
        print(f"ğŸ“… Rescheduled: {item['slug']} â†’ {slot}")

    # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å‰Šé™¤
    queue_data["queue"] = [item for item in items if item["retry_count"] >= MAX_RETRY]
    QUEUE_FILE.write_text(json.dumps(queue_data, indent=2, ensure_ascii=False))

    # Git commit
    import subprocess
    subprocess.run(["git", "add"] + [str(f) for f in updated_files], check=True)
    subprocess.run(["git", "add", str(QUEUE_FILE)], check=True)
    subprocess.run(["git", "commit", "-m", f"Reschedule {len(updated_files)} articles"], check=True)
    subprocess.run(["git", "push"], check=True)

if __name__ == "__main__":
    main()
```

### 4.3 GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

`.github/workflows/publish-with-retry.yml`:

```yaml
name: Publish with Retry

on:
  schedule:
    - cron: '0 */6 * * *'  # 6æ™‚é–“æ¯ã«å®Ÿè¡Œ
  workflow_dispatch:

jobs:
  publish:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install requests

      # Step 1: ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã‚’å‡¦ç†
      - name: Process retry queue
        run: python3 .github/scripts/zenn-retry-failed.py

      # Step 2: Zennå…¬é–‹å‡¦ç†
      - name: Publish scheduled articles
        uses: x-color/zenn-post-scheduler@v1.0.0
        with:
          username: correlate
          publish-if-today: true

      # Step 3: ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†ã‚’å¾…ã¤ï¼ˆ5åˆ†ï¼‰
      - name: Wait for Zenn deployment
        run: sleep 300

      # Step 4: æ¤œè¨¼ & ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
      - name: Verify published status
        run: python3 .github/scripts/zenn-verify-published.py
```

**ãƒã‚¤ãƒ³ãƒˆ**:
- 6æ™‚é–“ã”ã¨ã«è‡ªå‹•å®Ÿè¡Œï¼ˆcronï¼‰
- ãƒªãƒˆãƒ©ã‚¤å‡¦ç† â†’ å…¬é–‹ â†’ æ¤œè¨¼ã®é †ã§å®Ÿè¡Œ
- `sleep 300`ã§Zennã®ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†ã‚’å¾…ã¤

---

## ä»–ã®APIã¸ã®å¿œç”¨

ã“ã®4ã‚¹ãƒ†ãƒƒãƒ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã€Zennä»¥å¤–ã®APIã«ã‚‚é©ç”¨ã§ãã¾ã™ã€‚

### 5.1 Twitter API (X API)

**ãƒ¬ãƒ¼ãƒˆåˆ¶é™**:
- **Free Tier**: 50ãƒ„ã‚¤ãƒ¼ãƒˆ/æ—¥
- **åˆ¤å®šæ–¹å¼**: 24æ™‚é–“ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦

**é©ç”¨ä¾‹**:

```python
def check_tweet_published(tweet_id: str) -> bool:
    """ãƒ„ã‚¤ãƒ¼ãƒˆãŒå®Ÿéš›ã«æŠ•ç¨¿ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª"""
    url = f"https://api.twitter.com/2/tweets/{tweet_id}"
    headers = {"Authorization": f"Bearer {os.getenv('TWITTER_BEARER_TOKEN')}"}
    response = requests.get(url, headers=headers)
    return response.status_code == 200

def calculate_tweet_schedule(count: int) -> list[datetime]:
    """24æ™‚é–“ã«50æœ¬ä»¥å†…ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç”Ÿæˆ"""
    interval_minutes = (24 * 60) // 50  # ç´„29åˆ†é–“éš”
    slots = []
    current = datetime.now()

    for i in range(count):
        slots.append(current + timedelta(minutes=interval_minutes * i))

    return slots
```

### 5.2 GitHub API

**ãƒ¬ãƒ¼ãƒˆåˆ¶é™**:
- **èªè¨¼æ¸ˆã¿**: 5,000ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ™‚
- **ãƒ˜ãƒƒãƒ€ãƒ¼**: `X-RateLimit-Remaining`ã§æ®‹æ•°ç¢ºèªå¯èƒ½

**é©ç”¨ä¾‹**:

```python
def check_rate_limit_remaining() -> int:
    """GitHub APIã®æ®‹ã‚Šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’å–å¾—"""
    url = "https://api.github.com/rate_limit"
    headers = {"Authorization": f"token {os.getenv('GITHUB_TOKEN')}"}
    response = requests.get(url, headers=headers)
    return response.json()["resources"]["core"]["remaining"]

def wait_if_rate_limited():
    """ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆæ®‹æ•°ãŒå°‘ãªã„å ´åˆã¯å¾…æ©Ÿ"""
    remaining = check_rate_limit_remaining()
    if remaining < 100:
        reset_time = response.json()["resources"]["core"]["reset"]
        wait_seconds = reset_time - int(datetime.now().timestamp())
        print(f"â³ Rate limit low. Waiting {wait_seconds}s...")
        time.sleep(wait_seconds)
```

### 5.3 OpenAI API

**ãƒ¬ãƒ¼ãƒˆåˆ¶é™**:
- **RPM (Requests Per Minute)**: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹
- **TPM (Tokens Per Minute)**: ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹
- **429ã‚¨ãƒ©ãƒ¼**: ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¶…éæ™‚

**é©ç”¨ä¾‹ï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰**:

```python
import time

def call_openai_with_retry(prompt: str, max_retries: int = 3):
    """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§OpenAI APIã‚’ãƒªãƒˆãƒ©ã‚¤"""
    for attempt in range(max_retries):
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}]
            )
            return response
        except openai.error.RateLimitError:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # 1ç§’, 2ç§’, 4ç§’...
                print(f"â³ Rate limited. Retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise
```

---

## ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### 6.1 ç’°å¢ƒå¤‰æ•°ã§APIã‚­ãƒ¼ã‚’ç®¡ç†

**âŒ æ‚ªã„ä¾‹**:

```python
ZENN_USERNAME = "correlate"  # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰
GITHUB_TOKEN = "ghp_xxxxx"  # çµ¶å¯¾NG
```

**âœ… è‰¯ã„ä¾‹**:

```python
import os

ZENN_USERNAME = os.getenv("ZENN_USERNAME", "correlate")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

if not GITHUB_TOKEN:
    raise ValueError("GITHUB_TOKEN environment variable is required")
```

GitHub Actionsã§ã¯`secrets`ã‚’ä½¿ç”¨:

```yaml
env:
  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
  ZENN_USERNAME: ${{ secrets.ZENN_USERNAME }}
```

### 6.2 ãƒ­ã‚°è¨˜éŒ²

ãƒªãƒˆãƒ©ã‚¤å±¥æ­´ã‚’è¨˜éŒ²ã™ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("retry.log"),
        logging.StreamHandler()
    ]
)

logging.info(f"Rescheduled: {slug} â†’ {published_at}")
logging.error(f"Failed after {MAX_RETRY} retries: {slug}")
```

### 6.3 Discord/Slacké€šçŸ¥ã§ç•°å¸¸æ¤œçŸ¥

```python
import requests

def notify_discord(message: str):
    """Discord Webhookã§é€šçŸ¥"""
    webhook_url = os.getenv("DISCORD_WEBHOOK_URL")
    if not webhook_url:
        return

    requests.post(webhook_url, json={"content": message})

# ä½¿ç”¨ä¾‹
if failed_articles:
    notify_discord(f"âš ï¸  {len(failed_articles)} articles failed to publish")
```

### 6.4 ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆdry-runï¼‰ã®å®Ÿè£…

```python
def main(dry_run: bool = False):
    if dry_run:
        print("[DRY RUN] Would reschedule:", slug, "â†’", published_at)
        return

    # å®Ÿéš›ã®å‡¦ç†
    update_published_at(file_path, published_at)
```

å®Ÿè¡Œæ™‚:

```bash
python zenn-retry-failed.py --dry-run
```

---

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–ã¨ã—ã¦**4ã‚¹ãƒ†ãƒƒãƒ—ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³**ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚

### 4ã‚¹ãƒ†ãƒƒãƒ—ã®å¾©ç¿’

1. **æ¤œè¨¼ï¼ˆVerificationï¼‰**: APIã§å®Ÿéš›ã«æˆåŠŸã—ãŸã‹ç¢ºèª
2. **ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆRollbackï¼‰**: å¤±æ•—ã—ãŸçŠ¶æ…‹ã‚’å…ƒã«æˆ»ã™
3. **ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°ï¼ˆQueueingï¼‰**: ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã‚’è¨˜éŒ²
4. **ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆSchedulingï¼‰**: ç«¶åˆã‚’é¿ã‘ã¦å†å®Ÿè¡Œ

### ã“ã®è¨­è¨ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ©ç‚¹

- âœ… **å®Œå…¨è‡ªå‹•åŒ–**: äººé–“ã®ä»‹å…¥ä¸è¦
- âœ… **æ±ç”¨æ€§**: ã©ã®APIãƒ»ã©ã®è¨€èªã§ã‚‚å¿œç”¨å¯èƒ½
- âœ… **å†ªç­‰æ€§**: ä½•åº¦å®Ÿè¡Œã—ã¦ã‚‚å®‰å…¨
- âœ… **ç«¶åˆå›é¿**: æ—¢å­˜ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨é‡è¤‡ã—ãªã„
- âœ… **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: ãƒªãƒˆãƒ©ã‚¤å›æ•°åˆ¶é™ã€ãƒ­ã‚°è¨˜éŒ²

### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

- **æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•**: ãƒªãƒˆãƒ©ã‚¤é–“éš”ã‚’æ®µéšçš„ã«å¢—ã‚„ã™
- **ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼**: é€£ç¶šå¤±æ•—æ™‚ã«è‡ªå‹•åœæ­¢
- **åˆ†æ•£ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼**: Redis/RabbitMQã§è¤‡æ•°ãƒ¯ãƒ¼ã‚«ãƒ¼å¯¾å¿œ

ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–ã¯ã€APIåˆ©ç”¨ã®åŸºæœ¬ã‚¹ã‚­ãƒ«ã§ã™ã€‚å¤±æ•—ã‚’å‰æã¨ã—ãŸè¨­è¨ˆã§ã€å …ç‰¢ãªã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ã€‚

---

## å‚è€ƒè³‡æ–™

- [Zenn FAQ - ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆ](https://zenn.dev/faq)
- [GitHub REST API Rate Limits](https://docs.github.com/en/rest/overview/rate-limits)
- [Twitter API Rate Limits](https://developer.twitter.com/en/docs/twitter-api/rate-limits)
- [OpenAI API Error Codes](https://platform.openai.com/docs/guides/error-codes)
