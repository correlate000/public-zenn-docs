#!/usr/bin/env python3
"""
Zennå…¬é–‹çŠ¶æ…‹æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆGitHub Actionsç”¨ï¼‰

published: true ã®è¨˜äº‹ãŒå®Ÿéš›ã«Zennã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã€
å…¬é–‹ã•ã‚Œã¦ã„ãªã„è¨˜äº‹ã‚’ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã™ã‚‹ã€‚

ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–:
  - çŒ¶äºˆæœŸé–“: æœ€çµ‚å¤‰æ›´ã‹ã‚‰6æ™‚é–“ä»¥å†…ã®è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤å¾…ã¡ï¼‰
  - å¤±æ•—è¨˜éŒ²: ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã« .publish-failures.json ã«è¨˜éŒ²
  - daily-publish.py ãŒå¤±æ•—è¨˜äº‹ã‚’48hã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã§å›é¿

Usage:
    python3 zenn-verify-published.py [--fix]
"""

import json
import os
import re
import subprocess
import sys
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Dict, List

import requests

# è¨­å®šï¼ˆGitHub Actionsç’°å¢ƒå¯¾å¿œï¼‰
WORKSPACE = Path(os.getenv("GITHUB_WORKSPACE", "."))
ARTICLES_DIR = WORKSPACE / "articles"
RETRY_QUEUE_FILE = WORKSPACE / ".github/scripts/.zenn-retry-queue.json"
FAILURE_LOG = WORKSPACE / "scripts" / ".publish-failures.json"
DISCORD_WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL_CONTENT", "")
GRACE_PERIOD_HOURS = 6
JST = timezone(timedelta(hours=9))


def extract_front_matter(file_path: Path) -> Dict[str, str]:
    """front matterã‚’æŠ½å‡º"""
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    match = re.match(r"^---\n(.*?)\n---", content, re.DOTALL)
    if not match:
        return {}

    front_matter = {}
    for line in match.group(1).split("\n"):
        if ":" in line:
            key, value = line.split(":", 1)
            front_matter[key.strip()] = value.strip().strip('"')

    return front_matter


def check_published_on_zenn(slug: str, username: str = "correlate") -> bool | None:
    """Zennã§å®Ÿéš›ã«å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª. ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼æ™‚ã¯ None."""
    url = f"https://zenn.dev/{username}/articles/{slug}"
    try:
        response = requests.head(url, timeout=10, allow_redirects=True)
        return response.status_code in [200, 301]
    except Exception as e:
        print(f"  [WARN] Failed to check {slug}: {e}", file=sys.stderr)
        return None


def get_file_modified_hours_ago(file_path: Path) -> float | None:
    """git logã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚ã‚³ãƒŸãƒƒãƒˆæ™‚åˆ»ã‚’å–å¾—ã—ã€çµŒéæ™‚é–“ã‚’è¿”ã™"""
    try:
        result = subprocess.run(
            ["git", "log", "-1", "--format=%aI", "--", str(file_path)],
            capture_output=True,
            text=True,
            cwd=WORKSPACE,
            timeout=10,
        )
        if result.stdout.strip():
            modified = datetime.fromisoformat(result.stdout.strip())
            delta = datetime.now(timezone.utc) - modified
            return delta.total_seconds() / 3600
    except Exception:
        pass
    return None


def rollback_published_flag(file_path: Path):
    """published: true â†’ false ã«ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ + published_at ã‚‚å‰Šé™¤"""
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # published: true â†’ false
    updated = re.sub(
        r"^published:\s*true", "published: false", content, flags=re.MULTILINE
    )

    # published_at ã‚‚å‰Šé™¤ï¼ˆpublished: false + published_at ã¯ç„¡åŠ¹ãªçµ„ã¿åˆã‚ã›ï¼‰
    updated = re.sub(r"^published_at:.*\n", "", updated, flags=re.MULTILINE)

    with open(file_path, "w", encoding="utf-8") as f:
        f.write(updated)


def load_retry_queue() -> List[Dict]:
    """ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
    if not RETRY_QUEUE_FILE.exists():
        return []

    with open(RETRY_QUEUE_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_retry_queue(queue: List[Dict]):
    """ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã‚’ä¿å­˜"""
    RETRY_QUEUE_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(RETRY_QUEUE_FILE, "w", encoding="utf-8") as f:
        json.dump(queue, f, indent=2, ensure_ascii=False)


def load_failures() -> dict:
    """å¤±æ•—ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿"""
    if not FAILURE_LOG.exists():
        return {}
    try:
        with open(FAILURE_LOG, encoding="utf-8") as f:
            return json.load(f)
    except (json.JSONDecodeError, Exception):
        return {}


def save_failures(failures: dict):
    """å¤±æ•—ãƒ­ã‚°ã‚’ä¿å­˜"""
    FAILURE_LOG.parent.mkdir(parents=True, exist_ok=True)
    with open(FAILURE_LOG, "w", encoding="utf-8") as f:
        json.dump(failures, f, indent=2, ensure_ascii=False)


def record_failure(slug: str, failures: dict):
    """å¤±æ•—ã‚’è¨˜éŒ²ï¼ˆã‚«ã‚¦ãƒ³ãƒˆ+ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰"""
    now_str = datetime.now(JST).isoformat()
    if slug in failures:
        failures[slug]["count"] += 1
        failures[slug]["last_failed"] = now_str
    else:
        failures[slug] = {"count": 1, "last_failed": now_str}


def send_discord_notification(failed_articles: List[Dict]):
    """Discordé€šçŸ¥é€ä¿¡"""
    if not failed_articles or not DISCORD_WEBHOOK_URL:
        return

    article_list = "\n".join(
        [f"- `{a['slug']}` ({a['title'][:30]}...)" for a in failed_articles]
    )

    message = f"""ğŸš¨ **Zennå…¬é–‹å¤±æ•—æ¤œçŸ¥**

ä»¥ä¸‹ã®è¨˜äº‹ãŒ `published: true` ã§ã™ãŒã€Zennã«å…¬é–‹ã•ã‚Œã¦ã„ã¾ã›ã‚“:

{article_list}

**å¯¾å‡¦**: ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ + 48hã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³é©ç”¨æ¸ˆã¿ã€‚è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ã•ã‚Œã¾ã™ã€‚

**ç¢ºèª**: https://zenn.dev/dashboard
"""

    try:
        requests.post(
            DISCORD_WEBHOOK_URL, json={"content": message}, timeout=10
        )
    except Exception as e:
        print(f"Discordé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)


def main():
    fix_mode = "--fix" in sys.argv

    print(f"Zennå…¬é–‹çŠ¶æ…‹æ¤œè¨¼é–‹å§‹: {datetime.now(JST).isoformat()}")
    print(f"Articles dir: {ARTICLES_DIR}")
    print(f"Fix mode: {'ON' if fix_mode else 'OFF'}")
    print(f"Grace period: {GRACE_PERIOD_HOURS}h")

    failed_articles = []
    retry_queue = load_retry_queue()
    failures = load_failures()

    now = datetime.now(JST)

    for article_file in ARTICLES_DIR.glob("*.md"):
        front_matter = extract_front_matter(article_file)

        if front_matter.get("published") != "true":
            continue

        slug = front_matter.get("slug", article_file.stem)
        title = front_matter.get("title", slug)

        # published_at ãŒæœªæ¥ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆäºˆç´„å…¬é–‹å¾…ã¡ï¼‰
        published_at_str = front_matter.get("published_at", "")
        if published_at_str:
            try:
                published_at = datetime.strptime(
                    published_at_str, "%Y-%m-%d %H:%M"
                ).replace(tzinfo=JST)
                if published_at > now:
                    print(f"Checking: {slug}... â³ SCHEDULED ({published_at_str})")
                    continue
            except ValueError:
                pass  # ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯é€šå¸¸ãƒã‚§ãƒƒã‚¯

        # çŒ¶äºˆæœŸé–“: æœ€çµ‚ã‚³ãƒŸãƒƒãƒˆã‹ã‚‰6hä»¥å†…ã®è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—
        hours_ago = get_file_modified_hours_ago(article_file)
        if hours_ago is not None and hours_ago < GRACE_PERIOD_HOURS:
            print(
                f"Checking: {slug}... â³ GRACE PERIOD ({hours_ago:.1f}h ago, need {GRACE_PERIOD_HOURS}h)"
            )
            continue

        print(f"Checking: {slug}...", end=" ")

        result = check_published_on_zenn(slug)
        if result is True:
            # å…¬é–‹æˆåŠŸ â†’ å¤±æ•—ãƒ­ã‚°ã‹ã‚‰å‰Šé™¤
            if slug in failures:
                del failures[slug]
            print("âœ… OK")
        elif result is None:
            print("âš ï¸ NETWORK ERROR (skipped)")
        else:
            print("âŒ NOT PUBLISHED")
            failed_articles.append(
                {
                    "slug": slug,
                    "title": title,
                    "file": str(article_file.relative_to(WORKSPACE)),
                    "detected_at": now.isoformat(),
                }
            )

            # å¤±æ•—è¨˜éŒ²
            record_failure(slug, failures)

            if fix_mode:
                rollback_published_flag(article_file)
                print(f"  â†’ Rolled back to published: false")

    # å¤±æ•—ãƒ­ã‚°ã®è‚¥å¤§åŒ–é˜²æ­¢: å¤±æ•—10å›ä»¥ä¸Šã®è¨˜äº‹ã¯æ‰‹å‹•å¯¾å¿œæ¡ˆä»¶ã¨ã—ã¦é™¤å¤–
    permanent_failures = [
        slug for slug, data in failures.items() if data.get("count", 0) >= 10
    ]
    for slug in permanent_failures:
        print(f"  âš ï¸ {slug}: å¤±æ•—10å›ä»¥ä¸Š â†’ æ‰‹å‹•å¯¾å¿œãŒå¿…è¦ã§ã™")
        del failures[slug]

    # å¤±æ•—ãƒ­ã‚°ä¿å­˜
    save_failures(failures)

    # ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆ30æ—¥è¶…ã®ã‚¨ãƒ³ãƒˆãƒªã¯è‡ªå‹•å‰Šé™¤ï¼‰
    cutoff = (now - timedelta(days=30)).isoformat()
    retry_queue = [
        item for item in retry_queue
        if item.get("detected_at", now.isoformat()) > cutoff
    ]
    existing_slugs = {item["slug"] for item in retry_queue}
    for article in failed_articles:
        if article["slug"] not in existing_slugs:
            retry_queue.append(article)

    save_retry_queue(retry_queue)

    # Discordé€šçŸ¥
    if failed_articles:
        send_discord_notification(failed_articles)
        print(f"\nğŸš¨ {len(failed_articles)}ä»¶ã®å…¬é–‹å¤±æ•—ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸ")
        print(f"ãƒªãƒˆãƒ©ã‚¤ã‚­ãƒ¥ãƒ¼: {len(retry_queue)}ä»¶")
        print(f"å¤±æ•—è¨˜éŒ²: {len(failures)}ä»¶ï¼ˆ48hã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³é©ç”¨ï¼‰")
    else:
        print("\nâœ… å…¨è¨˜äº‹ãŒæ­£å¸¸ã«å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™")

    print(f"æ¤œè¨¼å®Œäº†: {datetime.now(JST).isoformat()}")


if __name__ == "__main__":
    main()
